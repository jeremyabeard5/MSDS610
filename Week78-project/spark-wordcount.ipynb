{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WordCount Step-by-Step using Spark\n",
    "\n",
    "Spark is a \"functional programming\" tool. What this means for us each line of code operates on the entire data set and transforms it in some way, returning the entire data set when finished. In order to illustrate the transformations that happen, we will read a large text file (the works of Shakespeare appended to itself several times) and show how we manipulate the data to arrive at a count of words in the file.\n",
    "\n",
    "\n",
    "Even though Jupyter notebooks can be setup to have a Spark \"kernel\" allowing us to create Spark notebooks, setting up the notebook server to find and use spark is... difficult. \n",
    "\n",
    "Thus enter the **findspark** library. If we tell it where our Spark directory is, it will find Spark and return the starting object we work from -- the \"spark context\" usually abbreviated \"sc.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('/usr/hdp/2.6.3.0-235/spark2')\n",
    "\n",
    "import pyspark\n",
    "sc = pyspark.SparkContext(appName=\"WordCount\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the spark context, let's use it to load our text data file from HDFS. This stores the data into a data structure called a **Resilient Distributed Dataset (RDD).** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_file = sc.textFile(\"hdfs:///user/vagrant/data/input/shake_large.txt\")\n",
    "\n",
    "# Check that text_file is an RDD\n",
    "from pyspark.rdd import RDD\n",
    "isinstance(text_file, RDD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Always, _Always_, **ALWAYS!**  look at your data!\n",
    "\n",
    "Spark doesn't *actually* load the entire data set when you evaluate the cell above. It waits until it absolutely has to. In order to look at the first few lines of the file you need to use the **take()** command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"A MIDSUMMER-NIGHT'S DREAM\",\n",
       " '',\n",
       " 'Now , fair Hippolyta , our nuptial hour ',\n",
       " 'Draws on apace : four happy days bring in ',\n",
       " 'Another moon ; but O ! methinks how slow ']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_file.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strategy ##\n",
    "\n",
    "Recall that in traditional MapReduce WordCount, the map() function will emit single words paired with the number 1. The reduce() then collects all of those words and sums them up, outputting the sum for each word into the output file(s). \n",
    "\n",
    "Right now, however, we do not have single words, we have lines of multiple words. So the first step of our strategy is to break the lines into individual words. Then we can pair each word with a 1 and finally sum the counts for each word.\n",
    "\n",
    "Our goal going into the reduce() is to have something like this:\n",
    "\n",
    "`[('A', 1), ('MIDSUMMER-NIGHT'S', 1), ('DREAM', 1), ('Now',1), ... ]`\n",
    "\n",
    "### 1) Break the lines into individual words\n",
    "For this step we use the map() function of RDDs. Recall that the map() function takes 2 arguments:\n",
    "* The function to apply to the data set\n",
    "* The data itself\n",
    "* Since our data is strings, we can do several \"clean up\" functions at once\n",
    "    * .lower() to make everything lower case\n",
    "    * .replace(',',' ') to replace commas with spaces\n",
    "In this case, the RDD word_list is the data set so we just need to provide a function. It is **very** common to use a **\"lambda function.\"** Lambda functions are just little one-liners like below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['a', \"midsummer-night's\", 'dream'],\n",
       " [],\n",
       " ['now', 'fair', 'hippolyta', 'our', 'nuptial', 'hour'],\n",
       " ['draws', 'on', 'apace', ':', 'four', 'happy', 'days', 'bring', 'in'],\n",
       " ['another', 'moon', ';', 'but', 'o', '!', 'methinks', 'how', 'slow']]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# x is an element of the data set - in this case a line of text\n",
    "word_list = text_file.map(lambda x: x.lower().replace(',',' ').split()) \n",
    "# BTW, there's nothing \"magic\" about x. it is just a variable. you can name it anything\n",
    "word_list.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A list of... lists?\n",
    "The output of the last map command is a little surprising. **What happened?**\n",
    "\n",
    "Recall that map() applies the function to each \"piece\" of data. In this case, each piece of data is a *line of text* and the function, split(), takes a line of text and **returns a list of individual words.** \n",
    "\n",
    "Our first transformation gave us individual words but they are wrapped in lists that we don't need. Fortunately, we have a way to deal with it. \n",
    "\n",
    "A **list of lists** is called **nested lists.** If you convert nested lists into a single list, you **flatten** it. \n",
    "\n",
    "Spark has a function that will act like map() and also flatten nested lists. It is called **flatmap().**\n",
    "\n",
    "Let's see if we can use flatmap() to output our (word, 1) pairs. BTW, the parentheses on the **(**word,1**)** pair means we are grouping each word with the number 1 (technically, this is called a \"tuple\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tuple = word_list.flatMap(lambda wordlist: [(word, 1) for word in wordlist])\n",
    "word_tuple.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OK, that is... complicated\n",
    "\n",
    "Let's look at that last line in detail.\n",
    "\n",
    "* `flatmap()` helps to flatten out the nested lists, and it takes a function just like `map()`. That's where the lambda comes in,\n",
    "* On this lambda I called the incoming data **wordlist** just to help distinguish it.\n",
    "\n",
    "\"OK,\" I hear you saying, \"but what the heck is with the brackets and `for` thingy?\"\n",
    "\n",
    "I'm glad you asked! The brackets make a **list comprehension**. This is a fancy, one-line version of a for loop.\n",
    "\n",
    "Observe, in regular Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ['Now', ',', 'fair', 'Hippolyta', ',', 'our', 'nuptial', 'hour']\n",
    "for word in data:\n",
    "    print ((word, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks just like what we want. Let's try it as a list comprehension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(word, 1) for word in data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the list comprehension gave us output in a list. The `flatmap()` function helped keep from nesting more lists.\n",
    "\n",
    "## Next up: reduce()\n",
    "\n",
    "\"Reduce\" in the MapReduce sense of the word means **to gather** or **to sum up.** \n",
    "\n",
    "You might remember that MapReduce has an *intermediate* shuffle/sort step that helps group all the occurrences of a word before going in to reduce. It effectivel turns this:\n",
    "\n",
    "`('the', 1), ('the', 1), ('the', 1), ('the', 1), ('the', 1)`\n",
    "\n",
    "into this:\n",
    "\n",
    "`('the', 1,1,1,1,1)`\n",
    "\n",
    "An interesting way of thinking of each tuple (thing with parentheses) is as a (key, value). So, above, each 'the' would be a key and each 1 is a value. Spark has a cool function called `reduceByKey()` that helps us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts = word_tuple.reduceByKey(lambda total, count: total + count)\n",
    "word_counts.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "That operation seems to have done exactly what we want. **BUT** even though that operation may have taken a long time, that's just because of the .take(). Normally, the full set of transformations would are not applied until a `reduce()` -type action is taken. Also a call to `collect()` will finalize transformations and collect the results *(can be used when you don't need a reduce but need finalize before (for example) writing to an output file)*. Let's check how many elements are in word_counts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving\n",
    "\n",
    "RDDs can be saved back to disk very easily. Just use the `saveAsTextFile()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts.saveAsTextFile('spark_wordcount.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# But Wait! There's More!\n",
    "\n",
    "Spark commands are meant to be **\"chained\"** together -- in other words, the output from one function call is immediately used as input for the next function call.\n",
    "\n",
    "Let's see how chaining would work on our WordCount example. Remember, the variable **text_file** holds the contents of our source data file.\n",
    "\n",
    "Here is the code, with comments:\n",
    "\n",
    "`text_file.map(lambda x: x.split()).  \\ # We put a \".\" after the map() to \"chain\" results -- \\ is a line continuation\n",
    "            flatMap(lambda wordlist: [(word, 1) for word in wordlist]). \\\n",
    "            reduceByKey(lambda total, count: total + count). \\\n",
    "            take(5) `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_file.map(lambda x: x.split()).      \\\n",
    "            flatMap(lambda wordlist: [(word, 1) for word in wordlist]). \\\n",
    "            reduceByKey(lambda total, count: total + count). \\\n",
    "            take(5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
