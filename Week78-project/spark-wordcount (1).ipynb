{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WordCount Step-by-Step using Spark\n",
    "\n",
    "Spark is a \"functional programming\" tool. What this means for us each line of code operates on the entire data set and transforms it in some way, returning the entire data set when finished. In order to illustrate the transformations that happen, we will read a large text file (the works of Shakespeare appended to itself several times) and show how we manipulate the data to arrive at a count of words in the file.\n",
    "\n",
    "\n",
    "Even though Jupyter notebooks can be setup to have a Spark \"kernel\" allowing us to create Spark notebooks, setting up the notebook server to find and use spark is... difficult. \n",
    "\n",
    "Thus enter the **findspark** library. If we tell it where our Spark directory is, it will find Spark and return the starting object we work from -- the \"spark context\" usually abbreviated \"sc.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the original version of the course, this was running on a simulated cluster on our local machines.  We had to use `findspark` to tell pyspark where to find pyspark.  The original findspark code is left here, but commented out, for completeness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import findspark\n",
    "#findspark.init('/usr/hdp/2.6.3.0-235/spark2')\n",
    "\n",
    "import pyspark\n",
    "# For pyspark kernel\n",
    "sc = pyspark.SparkContext.getOrCreate()\n",
    "# For Python3 kernel\n",
    "#sc = pyspark.SparkContext(appName=\"WordCount\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://cluster-6bac-m.us-east4-a.c.msds610.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=yarn appName=PySparkShell>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This allows us to load csvs and text files easily with spark.read.csv(path_to_file)\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the spark context, let's use it to load our text data file from HDFS. This stores the data into a data structure called a **Resilient Distributed Dataset (RDD).** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to download the shakespeare text we will use as a demo and put it into HDFS for spark to use.  This file is from here: https://norvig.com/ngrams/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-06-26 02:32:55--  https://norvig.com/ngrams/shakespeare.txt\n",
      "Resolving norvig.com (norvig.com)... 158.106.138.13\n",
      "Connecting to norvig.com (norvig.com)|158.106.138.13|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 4538523 (4.3M) [text/plain]\n",
      "Saving to: ‘shakespeare.txt.4’\n",
      "\n",
      "shakespeare.txt.4   100%[===================>]   4.33M  15.1MB/s    in 0.3s    \n",
      "\n",
      "2019-06-26 02:32:55 (15.1 MB/s) - ‘shakespeare.txt.4’ saved [4538523/4538523]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://norvig.com/ngrams/shakespeare.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: `/shake': File exists\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -mkdir /shake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "copyFromLocal: `/shake/shakespeare.txt': File exists\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -copyFromLocal shakespeare.txt /shake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\r\n",
      "-rw-r--r--   2 root hadoop    4538523 2019-06-26 01:31 /shake/shakespeare.txt\r\n",
      "drwxr-xr-x   - root hadoop          0 2019-06-26 02:29 /shake/spark_wordcount.txt\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /shake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can load the file into spark easily.  This can also be done like\n",
    "\n",
    "`text_file = sc.textFile('hdfs:///shake/shakespeare.txt')`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you wanted to load a lot of files from a data lake, we can use the same command:\n",
    "https://stackoverflow.com/a/24036343/4549682"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# text_file = sc.textFile('hdfs:///shake/shakespeare.txt')\n",
    "text_file = sc.textFile(\"/shake/shakespeare.txt\")\n",
    "\n",
    "# Check that text_file is an RDD\n",
    "from pyspark.rdd import RDD\n",
    "isinstance(text_file, RDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 items\r\n",
      "drwxr-xr-x   - root hadoop          0 2019-06-26 02:32 .sparkStaging\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Always, _Always_, **ALWAYS!**  look at your data!\n",
    "\n",
    "Spark doesn't *actually* load the entire data set when you evaluate the cell above. It waits until it absolutely has to. In order to look at the first few lines of the file you need to use the **take()** command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"A MIDSUMMER-NIGHT'S DREAM\",\n",
       " '',\n",
       " 'Now , fair Hippolyta , our nuptial hour ',\n",
       " 'Draws on apace : four happy days bring in ',\n",
       " 'Another moon ; but O ! methinks how slow ']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_file.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strategy ##\n",
    "\n",
    "Recall that in traditional MapReduce WordCount, the map() function will emit single words paired with the number 1. The reduce() then collects all of those words and sums them up, outputting the sum for each word into the output file(s). \n",
    "\n",
    "Right now, however, we do not have single words, we have lines of multiple words. So the first step of our strategy is to break the lines into individual words. Then we can pair each word with a 1 and finally sum the counts for each word.\n",
    "\n",
    "Our goal going into the reduce() is to have something like this:\n",
    "\n",
    "`[('A', 1), ('MIDSUMMER-NIGHT'S', 1), ('DREAM', 1), ('Now',1), ... ]`\n",
    "\n",
    "### 1) Break the lines into individual words\n",
    "For this step we use the map() function of RDDs. Recall that the map() function takes 2 arguments:\n",
    "* The function to apply to the data set\n",
    "* The data itself\n",
    "* Since our data is strings, we can do several \"clean up\" functions at once\n",
    "    * .lower() to make everything lower case\n",
    "    * .replace(',',' ') to replace commas with spaces\n",
    "In this case, the RDD word_list is the data set so we just need to provide a function. It is **very** common to use a **\"lambda function.\"** Lambda functions are just little one-liners like below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['a', \"midsummer-night's\", 'dream'],\n",
       " [],\n",
       " ['now', 'fair', 'hippolyta', 'our', 'nuptial', 'hour'],\n",
       " ['draws', 'on', 'apace', ':', 'four', 'happy', 'days', 'bring', 'in'],\n",
       " ['another', 'moon', ';', 'but', 'o', '!', 'methinks', 'how', 'slow']]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# x is an element of the data set - in this case a line of text\n",
    "word_list = text_file.map(lambda x: x.lower().replace(',',' ').split()) \n",
    "# BTW, there's nothing \"magic\" about x. it is just a variable. you can name it anything\n",
    "word_list.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A list of... lists?\n",
    "The output of the last map command is a little surprising. **What happened?**\n",
    "\n",
    "Recall that map() applies the function to each \"piece\" of data. In this case, each piece of data is a *line of text* and the function, split(), takes a line of text and **returns a list of individual words.** \n",
    "\n",
    "Our first transformation gave us individual words but they are wrapped in lists that we don't need. Fortunately, we have a way to deal with it. \n",
    "\n",
    "A **list of lists** is called **nested lists.** If you convert nested lists into a single list, you **flatten** it. \n",
    "\n",
    "Spark has a function that will act like map() and also flatten nested lists. It is called **flatmap().**\n",
    "\n",
    "Let's see if we can use flatmap() to output our (word, 1) pairs. BTW, the parentheses on the **(**word,1**)** pair means we are grouping each word with the number 1 (technically, this is called a \"tuple\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 1), (\"midsummer-night's\", 1), ('dream', 1), ('now', 1), ('fair', 1)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tuple = word_list.flatMap(lambda wordlist: [(word, 1) for word in wordlist])\n",
    "word_tuple.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OK, that is... complicated\n",
    "\n",
    "Let's look at that last line in detail.\n",
    "\n",
    "* `flatmap()` helps to flatten out the nested lists, and it takes a function just like `map()`. That's where the lambda comes in,\n",
    "* On this lambda I called the incoming data **wordlist** just to help distinguish it.\n",
    "\n",
    "\"OK,\" I hear you saying, \"but what the heck is with the brackets and `for` thingy?\"\n",
    "\n",
    "I'm glad you asked! The brackets make a **list comprehension**. This is a fancy, one-line version of a for loop.\n",
    "\n",
    "Observe, in regular Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Now', 1)\n",
      "(',', 1)\n",
      "('fair', 1)\n",
      "('Hippolyta', 1)\n",
      "(',', 1)\n",
      "('our', 1)\n",
      "('nuptial', 1)\n",
      "('hour', 1)\n"
     ]
    }
   ],
   "source": [
    "data = ['Now', ',', 'fair', 'Hippolyta', ',', 'our', 'nuptial', 'hour']\n",
    "for word in data:\n",
    "    print ((word, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks just like what we want. Let's try it as a list comprehension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Now', 1),\n",
       " (',', 1),\n",
       " ('fair', 1),\n",
       " ('Hippolyta', 1),\n",
       " (',', 1),\n",
       " ('our', 1),\n",
       " ('nuptial', 1),\n",
       " ('hour', 1)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(word, 1) for word in data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the list comprehension gave us output in a list. The `flatmap()` function helped keep from nesting more lists.\n",
    "\n",
    "## Next up: reduce()\n",
    "\n",
    "\"Reduce\" in the MapReduce sense of the word means **to gather** or **to sum up.** \n",
    "\n",
    "You might remember that MapReduce has an *intermediate* shuffle/sort step that helps group all the occurrences of a word before going in to reduce. It effectivel turns this:\n",
    "\n",
    "`('the', 1), ('the', 1), ('the', 1), ('the', 1), ('the', 1)`\n",
    "\n",
    "into this:\n",
    "\n",
    "`('the', 1,1,1,1,1)`\n",
    "\n",
    "An interesting way of thinking of each tuple (thing with parentheses) is as a (key, value). So, above, each 'the' would be a key and each 1 is a value. Spark has a cool function called `reduceByKey()` that helps us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('his', 6529),\n",
       " ('world', 608),\n",
       " ('of', 16013),\n",
       " ('orleans', 28),\n",
       " ('bastard', 62)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_counts = word_tuple.reduceByKey(lambda total, count: total + count)\n",
    "word_counts.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "That operation seems to have done exactly what we want. **BUT** even though that operation may have taken a long time, that's just because of the .take(). Normally, the full set of transformations would are not applied until a `reduce()` -type action is taken. Also a call to `collect()` will finalize transformations and collect the results *(can be used when you don't need a reduce but need finalize before (for example) writing to an output file)*. Let's check how many elements are in word_counts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28687"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_counts.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving\n",
    "\n",
    "RDDs can be saved back to disk very easily. Just use the `saveAsTextFile()` function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If this file already exists, it will throw an error. It can be deleted with the command:\n",
    "\n",
    "`hdfs dfs -rm /shake/spark_wordcount`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `/shake/spark_wordcount': No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r /shake/spark_wordcount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts.saveAsTextFile('/shake/spark_wordcount')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\r\n",
      "-rw-r--r--   2 root hadoop    4538523 2019-06-26 01:31 /shake/shakespeare.txt\r\n",
      "drwxr-xr-x   - root hadoop          0 2019-06-26 02:34 /shake/spark_wordcount\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /shake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 items\n",
      "-rw-r--r--   2 root hadoop          0 2019-06-26 02:34 /shake/spark_wordcount/_SUCCESS\n",
      "-rw-r--r--   2 root hadoop     229535 2019-06-26 02:34 /shake/spark_wordcount/part-00000\n",
      "-rw-r--r--   2 root hadoop     225058 2019-06-26 02:34 /shake/spark_wordcount/part-00001\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /shake/spark_wordcount"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is another way to save a file.  A function that might be helpful is spark's `coalesce()`.  It collapses the RDD into a single partition so it is written to disk as a single file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember we can't have an existing file/folder when we write it to HDFS, so we delete it first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /testoutput\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r /testoutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading a file in with spark.read.csv works much better for csv files.\n",
    "text_df = spark.read.csv('/shake/shakespeare.txt')\n",
    "text_df.coalesce(1).write.format('json').save('/testoutput')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\n",
      "-rw-r--r--   2 root hadoop          0 2019-06-26 02:34 /testoutput/_SUCCESS\n",
      "-rw-r--r--   2 root hadoop    3782833 2019-06-26 02:34 /testoutput/part-00000-4686c0b3-0111-4763-b027-2af2f5fc5cd3-c000.json\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /testoutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6 items\r\n",
      "drwx------   - mapred hadoop          0 2019-06-26 01:18 /hadoop\r\n",
      "drwxr-xr-x   - root   hadoop          0 2019-06-26 02:34 /shake\r\n",
      "drwxr-xr-x   - root   hadoop          0 2019-06-26 02:27 /testfile.json\r\n",
      "drwxr-xr-x   - root   hadoop          0 2019-06-26 02:34 /testoutput\r\n",
      "drwxrwxrwt   - hdfs   hadoop          0 2019-06-26 01:18 /tmp\r\n",
      "drwxrwxrwt   - hdfs   hadoop          0 2019-06-26 01:30 /user\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, you can also use Python's built-in saving, or convert an RDD to a pandas dataframe with `toPandas()` (which requires a DF in spark I think: https://stackoverflow.com/a/48111699/4549682)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pandas_df = text_df.toPandas()\n",
    "pandas_df.to_csv('path_to_file.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is one way with base Python to save data, if you had json data: https://stackoverflow.com/a/12309296/4549682."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# But Wait! There's More!\n",
    "\n",
    "Spark commands are meant to be **\"chained\"** together -- in other words, the output from one function call is immediately used as input for the next function call.\n",
    "\n",
    "Let's see how chaining would work on our WordCount example. Remember, the variable **text_file** holds the contents of our source data file.\n",
    "\n",
    "Here is the code, with comments:\n",
    "\n",
    "`text_file.map(lambda x: x.split()).  \\ # We put a \".\" after the map() to \"chain\" results -- \\ is a line continuation\n",
    "            flatMap(lambda wordlist: [(word, 1) for word in wordlist]). \\\n",
    "            reduceByKey(lambda total, count: total + count). \\\n",
    "            take(5) `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"MIDSUMMER-NIGHT'S\", 1),\n",
       " ('Now', 741),\n",
       " ('Hippolyta', 6),\n",
       " ('nuptial', 21),\n",
       " ('apace', 25)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_file.map(lambda x: x.split()).      \\\n",
    "            flatMap(lambda wordlist: [(word, 1) for word in wordlist]). \\\n",
    "            reduceByKey(lambda total, count: total + count). \\\n",
    "            take(5) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
